//
//  Created by VT on 16.11.25.
//  Copyright © 2025 Volker Thieme. All rights reserved.
//
//  Permission is hereby granted, free of charge, to any person obtaining a copy
//  of this software and associated documentation files (the "Software"), to deal
//  in the Software without restriction, including without limitation the rights
//  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
//  copies of the Software, and to permit persons to whom the Software is
//  furnished to do so, subject to the following conditions:
//
//  The above copyright notice and this permission notice shall be included in
//  all copies or substantial portions of the Software.
//
//  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
//  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
//  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
//  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
//  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
//  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
//  THE SOFTWARE.
//  

/// Maximum likelihood fitter for the NegativeBinomial distribution in SwiftyStats.

import SwiftyStatsPrelude

extension MLEFitter {
    /// Fits a negative binomial distribution by maximum likelihood.
    ///
    /// Parameterization
    /// - We use “success–probability” form with:
    ///   - `r > 0`: number of successes (shape/size, not necessarily integer),
    ///   - `p ∈ (0, 1)`: probability of success.
    /// - Support for the observation `X` is `{0, 1, 2, …}` (count of failures before `r` successes).
    ///
    /// Probability mass function
    /// - For `x ∈ {0,1,2,…}`, `r > 0`, `p ∈ (0,1)`:
    ///   P(X = x | r, p) = C(x + r − 1, x) · (1 − p)^x · p^r,
    ///   where `C(·,·)` is the binomial coefficient generalized by gamma functions when `r` is non-integer.
    ///
    /// Log-likelihood
    /// - Given observations `x₁,…,x_n`, the log-likelihood is
    ///   ℓ(r, p) = Σ [ log C(x_i + r − 1, x_i) + x_i · log(1 − p) + r · log p ].
    ///
    /// Implementation details
    /// - Constraints `r > 0` and `p ∈ (0, 1)` are enforced via the solver’s transforms generated by
    ///   `makeParamSpecs(for: .negative_binomial, data:)`.
    /// - Per-observation log-pmf and analytic score are delegated to
    ///   `SwiftyBoost.Distribution.NegativeBinomial(successes: r, probabilityOfSuccess: p)`.
    /// - By default, this wrapper enables covariance estimation (θ-space) and diagnostics, and uses `.lbfgs`.
    ///
    /// Inputs
    /// - `data`: Non-empty sample of counts. Mathematically, each element should be an integer `x ≥ 0`.
    ///   Invalid inputs (e.g., negative or non-integer) are rejected by the underlying distribution and penalized.
    ///
    /// - Parameters:
    ///   - data: Observations `x_i ∈ {0,1,2,…}`. Must be non-empty.
    ///   - optimizer: Local optimizer (`.nelderMead`, `.bfgs`, `.lbfgs`). Default is `.lbfgs`.
    ///   - options: Optional solver options. If `nil`, sensible defaults are used and adjusted for covariance and diagnostics.
    ///
    /// - Returns: An `MLEResult<T>` with:
    ///   - `thetaHat = [r̂, p̂]`,
    ///   - `logLik` (maximized log-likelihood),
    ///   - convergence flags, evaluation counts,
    ///   - optional covariance and diagnostics when enabled.
    ///
    /// - Precondition: `data` must be non-empty.
    public static func fitNegativeBinomial(
        _ data: [T],
        optimizer: OptimizerKind = .lbfgs,
        options: MLEOptimizationOpts<T>? = nil
    ) -> MLEResult<T> {
        precondition(!data.isEmpty)
        let dummy = MLEProblem<T>(data: data, logpdf: {_,_ in 0}, paramSpecs: [.init(.real, initial: 0)])
        let specs = dummy.makeParamSpecs(for: .negative_binomial, data: data)

        let logPDF: @Sendable (T, [T]) -> T = { x, theta in
            let r = theta[0]
            let p  = theta[1]
            if !(r > .zero && p > .zero && p < T.one && x >= .zero) { return T.infinity }
            do {
                return try SwiftyBoost.Distribution.NegativeBinomial(successes: r, probabilityOfSuccess: p).logPdf(x)
            } catch {
                return T.infinity
            }
        }
        
        let gradLogPDF: @Sendable (T, [T]) -> [T] = { x, theta in
            let r = theta[0]
            let p  = theta[1]
            if !(r > .zero && p > .zero && p < T.one && x >= .zero) { return [T.nan, T.nan] }
            do {
                let dist = try SwiftyBoost.Distribution.NegativeBinomial(successes: r, probabilityOfSuccess: p)
                let res = dist.score(x: x, r: r, p: p)
                return [res.dr, res.dp]
            } catch {
                return [T.nan, T.nan]
            }
        }
        
        let problem = MLEProblem<T>(
            data: data,
            logpdf: logPDF,
            gradlogpdf: gradLogPDF,
            paramSpecs: specs
        )
        
        let opts: MLEOptimizationOpts<T> = {
            if let o = options { return o }
            var o = MLEOptimizationOpts<T>()
            o.computeCovariance = true
            o.optimizer = optimizer          // .lbfgs recommended
            o.multiStartCount = 4
            o.multiStartDesign = .lhs
            o.randomRestartCount = 1
            o.initialStepStrategy = .relativeTheta(T(0.25))
            o.gradStep = T(1e-5)
            o.hessianStep = T(5e-4)
            o.relTolLogLik = T(1e-7)
            o.diagnosticsEnabled = true
            return o
        }()
        
        return MLESolver.fit(problem, options: opts)
        
    }
}
